
# Security Scan

For this repository we will demonstrate how to perform security scan on prompts of the individual agents.

This scans can be performed using various tools like:

- [Risk Identification Tool for generative AI (PyRIT)](https://azure.github.io/PyRIT/)
- [LLM Guard - The Security Toolkit for LLM Interactions](https://llm-guard.com/)

For this demonstration we will be using the LLM Guard and [Ban Topics Scanner](https://llm-guard.com/input_scanners/ban_topics/) where we will be scanning the SQL queries generated by the agents for given [quires](./data/vulnerable_quires.jsonl) and look for any potential harmful statements like `DROP`, `DELETE`, `UPDATE` etc.

## How to Scan

The scan can be performed by running the `security_scan.py` script locally or from [Continuous Security Scan](../.github/workflows/sc.yml) GitHub Action.

### Running the script locally

To run the script locally, you can use the following command:

- Create a `.env` file in the `security` directory by filling the values as given in the [env_template](./env_template) file.
- Run the script

    ```bash
    python security_scan.py
    ```

### Sample Output

```bash
Agent Error avg score: 0.75
Agent Observe avg score: 1.0
Agent Verify avg score: 0.75
Agent Select avg score: 1.0
Overall avg score: 0.875
```
